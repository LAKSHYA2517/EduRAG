{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b5c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Imports and Environment Setup ---\n",
    "import os\n",
    "import re\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# Added loaders for Word, CSV, and Excel document support\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader, CSVLoader, UnstructuredExcelLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86087535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "# Make sure you have a .env file with your GOOGLE_API_KEY\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Configuration ---\n",
    "DOCS_PATH = \"./documents\"\n",
    "CHROMA_PERSIST_PATH = \"./chroma_db\"\n",
    "EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "LLM_MODEL = \"gemini-1.5-flash-latest\" \n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3930698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Helper Functions (Document Processors, Vector Store, and RAG Chain Creation) ---\n",
    "\n",
    "class SmartPDFProcessor:\n",
    "    \"\"\"Advanced PDF processing with error handling\"\"\"\n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        )\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> List[Document]:\n",
    "        \"\"\"Process PDF with smart chunking and metadata enhancement\"\"\"\n",
    "        print(f\"Processing PDF: {pdf_path}\")\n",
    "        try:\n",
    "            # Load PDF\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            pages = loader.load()\n",
    "\n",
    "            # Process each page\n",
    "            processed_chunks = []\n",
    "            for page_num, page in enumerate(pages):\n",
    "                # Clean text\n",
    "                cleaned_text = self._clean_text(page.page_content)\n",
    "\n",
    "                # Skip nearly empty pages\n",
    "                if len(cleaned_text.strip()) < 50:\n",
    "                    continue\n",
    "\n",
    "                # Create chunks with enhanced metadata\n",
    "                chunks = self.text_splitter.create_documents(\n",
    "                    texts=[cleaned_text],\n",
    "                    metadatas=[{\n",
    "                        **page.metadata,\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"total_pages\": len(pages),\n",
    "                        \"chunk_method\": \"smart_pdf_processor\",\n",
    "                        \"char_count\": len(cleaned_text)\n",
    "                    }]\n",
    "                )\n",
    "                processed_chunks.extend(chunks)\n",
    "            \n",
    "            print(f\"Successfully processed {len(processed_chunks)} chunks from {pdf_path}\")\n",
    "            return processed_chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean extracted text from PDFs.\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Fix common PDF extraction issues (ligatures)\n",
    "        text = text.replace(\"ﬁ\", \"fi\")\n",
    "        text = text.replace(\"ﬂ\", \"fl\")\n",
    "        \n",
    "        return text\n",
    "\n",
    "class SmartDocProcessor:\n",
    "    \"\"\"Handles processing of various document types like .txt and .docx.\"\"\"\n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        )\n",
    "\n",
    "    def process_document(self, doc_path: str) -> List[Document]:\n",
    "        \"\"\"Loads, cleans, and chunks a document (.txt, .docx).\"\"\"\n",
    "        print(f\"Processing document: {doc_path}\")\n",
    "        try:\n",
    "            if doc_path.lower().endswith(\".docx\"):\n",
    "                loader = Docx2txtLoader(doc_path)\n",
    "            elif doc_path.lower().endswith(\".txt\"):\n",
    "                loader = TextLoader(doc_path, encoding='utf-8')\n",
    "            else:\n",
    "                print(f\"Unsupported file type: {doc_path}\")\n",
    "                return []\n",
    "\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Clean and filter documents\n",
    "            cleaned_docs = []\n",
    "            for doc in documents:\n",
    "                cleaned_text = self._clean_text(doc.page_content)\n",
    "                if len(cleaned_text.strip()) < 50:\n",
    "                    continue\n",
    "                # Create a new Document to avoid modifying the original\n",
    "                new_doc = Document(page_content=cleaned_text, metadata=doc.metadata.copy())\n",
    "                cleaned_docs.append(new_doc)\n",
    "\n",
    "            splits = self.text_splitter.split_documents(cleaned_docs)\n",
    "            \n",
    "            # Add extra metadata to each chunk\n",
    "            for split in splits:\n",
    "                split.metadata.update({\n",
    "                    \"chunk_method\": \"smart_doc_processor\",\n",
    "                    \"char_count\": len(split.page_content)\n",
    "                })\n",
    "\n",
    "            print(f\"Successfully processed {len(splits)} chunks from {doc_path}\")\n",
    "            return splits\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {doc_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Cleans extracted text from documents.\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "class SmartSheetProcessor:\n",
    "    \"\"\"Handles processing of spreadsheet files like .csv and .xlsx.\"\"\"\n",
    "    def process_sheet(self, sheet_path: str) -> List[Document]:\n",
    "        \"\"\"Loads and processes a spreadsheet file, treating rows as documents.\"\"\"\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        try:\n",
    "            if sheet_path.lower().endswith(\".csv\"):\n",
    "                loader = CSVLoader(file_path=sheet_path, encoding='utf-8')\n",
    "            elif sheet_path.lower().endswith(\".xlsx\"):\n",
    "                # mode=\"elements\" is good for unstructured tables\n",
    "                # mode=\"single\" might be better if each sheet is a cohesive document\n",
    "                loader = UnstructuredExcelLoader(sheet_path, mode=\"elements\")\n",
    "            else:\n",
    "                print(f\"Unsupported sheet type: {sheet_path}\")\n",
    "                return []\n",
    "\n",
    "            documents = loader.load()\n",
    "            \n",
    "            processed_docs = []\n",
    "            for doc in documents:\n",
    "                cleaned_content = self._clean_text(doc.page_content)\n",
    "                if len(cleaned_content.strip()) < 10: # Rows can be short but not empty\n",
    "                    continue\n",
    "                \n",
    "                doc.metadata.update({\n",
    "                    \"chunk_method\": \"smart_sheet_processor\",\n",
    "                    \"char_count\": len(cleaned_content)\n",
    "                })\n",
    "                doc.page_content = cleaned_content\n",
    "                processed_docs.append(doc)\n",
    "\n",
    "            print(f\"Successfully processed {len(processed_docs)} rows/elements from {sheet_path}\")\n",
    "            return processed_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {sheet_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Cleans extracted text from sheets.\"\"\"\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorstore(embedding_function):\n",
    "    \"\"\"Initializes and returns a Chroma vector store using smart processors.\"\"\"\n",
    "    if os.path.exists(CHROMA_PERSIST_PATH):\n",
    "        print(\"Loading existing vector store...\")\n",
    "        return Chroma(persist_directory=CHROMA_PERSIST_PATH, embedding_function=embedding_function)\n",
    "    else:\n",
    "        print(\"Creating new vector store...\")\n",
    "        all_splits = []\n",
    "        pdf_processor = SmartPDFProcessor()\n",
    "        doc_processor = SmartDocProcessor()\n",
    "        sheet_processor = SmartSheetProcessor() # New processor\n",
    "        \n",
    "        # Process all files in the documents directory\n",
    "        for filename in os.listdir(DOCS_PATH):\n",
    "            file_path = os.path.join(DOCS_PATH, filename)\n",
    "            \n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                all_splits.extend(pdf_processor.process_pdf(file_path))\n",
    "            elif filename.lower().endswith((\".txt\", \".docx\")):\n",
    "                all_splits.extend(doc_processor.process_document(file_path))\n",
    "            elif filename.lower().endswith((\".csv\", \".xlsx\")):\n",
    "                all_splits.extend(sheet_processor.process_sheet(file_path))\n",
    "\n",
    "        if not all_splits:\n",
    "            raise ValueError(f\"No processable documents found in {DOCS_PATH}. Please add your curriculum files.\")\n",
    "\n",
    "        # Create and persist the vector store\n",
    "        print(f\"Creating vector store with {len(all_splits)} document chunks...\")\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=all_splits,\n",
    "            embedding=embedding_function,\n",
    "            persist_directory=CHROMA_PERSIST_PATH\n",
    "        )\n",
    "        print(\"Vector store created successfully.\")\n",
    "        return vectorstore\n",
    "\n",
    "def create_conversational_rag_chain(retriever, llm):\n",
    "    \"\"\"Creates the conversational RAG chain.\"\"\"\n",
    "    # --- Contextualizing the Question ---\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    # --- Answering the Question ---\n",
    "    qa_system_prompt = (\n",
    "        \"You are an expert AI Curriculum Assistant. Your task is to answer user questions \"\n",
    "        \"accurately and concisely based ONLY on the provided context from the user's uploaded documents. \"\n",
    "        \"If the context does not contain the answer, state that you cannot find the information \"\n",
    "        \"in the provided materials. You may use any external knowledge. \"\n",
    "        \"Be friendly, helpful, and clear in your response.\\n\\n\"\n",
    "        \"Context:\\n{context}\"\n",
    "    )\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Initialization ---\n",
    "\n",
    "print(\"--- AI Curriculum Assistant ---\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"Error: GOOGLE_API_KEY not found. Please set it in your .env file.\")\n",
    "else:\n",
    "    # Initialize embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    \n",
    "    # Get or create the vector store\n",
    "    try:\n",
    "        vectorstore = get_vectorstore(embeddings)\n",
    "        \n",
    "        # Initialize the Gemini LLM with the API key\n",
    "        llm = ChatGoogleGenerativeAI(model=LLM_MODEL, google_api_key=GOOGLE_API_KEY)\n",
    "        \n",
    "        # Create the retriever\n",
    "        retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "        # Create the conversational RAG chain\n",
    "        rag_chain = create_conversational_rag_chain(retriever, llm)\n",
    "        \n",
    "        # Initialize chat history\n",
    "        chat_history = []\n",
    "        \n",
    "        print(\"\\nAssistant is ready! You can now ask questions in the next cell.\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during initialization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Interactive Q&A ---\n",
    "# To ask a new question, simply change the `user_input` variable and re-run this cell.\n",
    "\n",
    "user_input = \"What is the main topic of the documents?\"\n",
    "\n",
    "if 'rag_chain' in locals():\n",
    "    # Invoke the chain with the user's input and history\n",
    "    response = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
    "    \n",
    "    # Print the answer and update history\n",
    "    answer = response[\"answer\"]\n",
    "    print(f\"Assistant: {answer}\")\n",
    "    \n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    chat_history.append(AIMessage(content=answer))\n",
    "else:\n",
    "    print(\"The RAG chain is not initialized. Please run the previous cells successfully.\")\n",
    "\n",
    "# You can check the chat history by running a new cell with:\n",
    "# print(chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
