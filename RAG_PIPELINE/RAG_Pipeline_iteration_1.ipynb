{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4b5c322",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NougatParser' from 'langchain_community.document_loaders' (c:\\Users\\aryan\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ⬇️ ADDED: NougatParser for advanced PDF processing\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader, TextLoader, Docx2txtLoader, CSVLoader, UnstructuredExcelLoader, UnstructuredPowerPointLoader, NougatParser\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BM25Retriever\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EnsembleRetriever\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'NougatParser' from 'langchain_community.document_loaders' (c:\\Users\\aryan\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Imports and Environment Setup ---\n",
    "import os\n",
    "import re\n",
    "import base64\n",
    "import io\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# ⬇️ ADDED: NougatParser for advanced PDF processing\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader, CSVLoader, UnstructuredExcelLoader, UnstructuredPowerPointLoader, NougatParser\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "# ⬇️ MODIFIED: Reverted to Google Gemini for the LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from sentence_transformers.cross_encoder import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86087535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Configuration ---\n",
    "DOCS_PATH = \"./documents\"\n",
    "CHROMA_PERSIST_PATH = \"./chroma_db\"\n",
    "EMBEDDING_MODEL = 'BAAI/bge-base-en-v1.5'\n",
    "CROSS_ENCODER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
    "# ⬇️ MODIFIED: Reverted to Google Gemini models\n",
    "LLM_MODEL = \"gemini-1.5-flash-latest\" \n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Helper Functions ---\n",
    "\n",
    "# ⬇️ RE-ENABLED: Image summarization function using Gemini Vision\n",
    "def get_image_summary(image_bytes: bytes, llm: ChatGoogleGenerativeAI) -> str:\n",
    "    \"\"\"Generates a summary for an image using a multi-modal LLM.\"\"\"\n",
    "    print(\"Generating image summary...\")\n",
    "    prompt_messages = [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": \"You are an expert at analyzing academic images, diagrams, and charts. Describe this image in detail. What is its main purpose? What key information does it convey? If it's a chart or graph, describe the data, axes, and trend. This summary will be used for a Retrieval-Augmented Generation (RAG) system, so be comprehensive.\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{base64.b64encode(image_bytes).decode()}\"}\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    try:\n",
    "        response = llm.invoke(prompt_messages)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating image summary: {e}\")\n",
    "        return \"Could not generate summary for this image.\"\n",
    "\n",
    "class SmartPDFProcessor:\n",
    "    \"\"\"Advanced PDF processing using Nougat for text and PyMuPDF for images.\"\"\"\n",
    "    def __init__(self, embeddings, llm=None):\n",
    "        self.text_splitter = SemanticChunker(embeddings)\n",
    "        self.llm = llm\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> List[Document]:\n",
    "        \"\"\"Process PDF, extracting text with Nougat and images for summarization.\"\"\"\n",
    "        print(f\"Processing PDF with Nougat: {pdf_path}\")\n",
    "        all_docs = []\n",
    "        try:\n",
    "            # --- Text Extraction with Nougat ---\n",
    "            # Nougat is specialized for academic papers and handles complex layouts.\n",
    "            nougat_parser = NougatParser(file_path=pdf_path)\n",
    "            nougat_docs = nougat_parser.load()\n",
    "            \n",
    "            full_text = \"\\n\\n\".join([self._clean_text(doc.page_content) for doc in nougat_docs])\n",
    "            chunks = self.text_splitter.create_documents([full_text])\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata['source'] = pdf_path\n",
    "            all_docs.extend(chunks)\n",
    "\n",
    "            # --- Image Extraction and Summarization with Gemini Vision ---\n",
    "            if self.llm:\n",
    "                print(f\"Extracting images from: {pdf_path}\")\n",
    "                pdf_document = fitz.open(pdf_path)\n",
    "                for page_num in range(len(pdf_document)):\n",
    "                    for img_index, img in enumerate(pdf_document.get_page_images(page_num)):\n",
    "                        xref = img[0]\n",
    "                        base_image = pdf_document.extract_image(xref)\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        \n",
    "                        summary = get_image_summary(image_bytes, self.llm)\n",
    "                        image_doc = Document(\n",
    "                            page_content=summary,\n",
    "                            metadata={ \"source\": pdf_path, \"page\": page_num + 1, \"chunk_method\": \"nougat_pdf_image_summary\", \"image_index\": img_index }\n",
    "                        )\n",
    "                        all_docs.append(image_doc)\n",
    "\n",
    "            print(f\"✅ Successfully processed {len(all_docs)} text chunks and image summaries from {pdf_path}\")\n",
    "            return all_docs\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {pdf_path} with Nougat: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text.replace(\"ﬁ\", \"fi\").replace(\"ﬂ\", \"fl\")\n",
    "\n",
    "class SmartDocProcessor:\n",
    "    \"\"\"Handles .txt and .docx files using Semantic Chunking.\"\"\"\n",
    "    def __init__(self, embeddings):\n",
    "        self.text_splitter = SemanticChunker(embeddings)\n",
    "    def process_document(self, doc_path: str) -> List[Document]:\n",
    "        print(f\"Processing document: {doc_path}\")\n",
    "        try:\n",
    "            if doc_path.lower().endswith(\".docx\"): loader = Docx2txtLoader(doc_path)\n",
    "            elif doc_path.lower().endswith(\".txt\"): loader = TextLoader(doc_path, encoding='utf-8')\n",
    "            else: return []\n",
    "            documents = loader.load()\n",
    "            full_text = \"\\n\\n\".join([self._clean_text(doc.page_content) for doc in documents if len(self._clean_text(doc.page_content).strip()) >= 50])\n",
    "            if not full_text: return []\n",
    "            splits = self.text_splitter.create_documents([full_text])\n",
    "            for split in splits:\n",
    "                split.metadata.update({ \"source\": doc_path, \"chunk_method\": \"semantic_chunker_text\", \"char_count\": len(split.page_content)})\n",
    "            print(f\"✅ Successfully processed {len(splits)} chunks from {doc_path}\")\n",
    "            return splits\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {doc_path}: {e}\")\n",
    "            return []\n",
    "    def _clean_text(self, text: str) -> str: return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "class SmartLatexProcessor:\n",
    "    \"\"\"Handles .tex files, cleans LaTeX commands, and uses Semantic Chunking.\"\"\"\n",
    "    def __init__(self, embeddings):\n",
    "        self.text_splitter = SemanticChunker(embeddings)\n",
    "    def process_latex(self, tex_path: str) -> List[Document]:\n",
    "        print(f\"Processing LaTeX file: {tex_path}\")\n",
    "        try:\n",
    "            loader = TextLoader(tex_path, encoding='utf-8')\n",
    "            documents = loader.load()\n",
    "            full_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "            cleaned_text = self._clean_latex(full_text)\n",
    "            if len(cleaned_text.strip()) < 100: return []\n",
    "            splits = self.text_splitter.create_documents([cleaned_text])\n",
    "            for split in splits:\n",
    "                split.metadata.update({ \"source\": tex_path, \"chunk_method\": \"semantic_chunker_latex\", \"char_count\": len(split.page_content)})\n",
    "            print(f\"✅ Successfully processed {len(splits)} semantic chunks from {tex_path}\")\n",
    "            return splits\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {tex_path}: {e}\")\n",
    "            return []\n",
    "    def _clean_latex(self, text: str) -> str:\n",
    "        if \"\\\\begin{document}\" in text: text = text.split(\"\\\\begin{document}\")[1]\n",
    "        text = re.sub(r\"%.*?\\n\", \"\\n\", text)\n",
    "        text = re.sub(r\"\\\\begin\\{(?:figure|table|tabular|verbatim|lstlisting)\\*?\\}[\\s\\S]*?\\\\end\\{(?:figure|table|tabular|verbatim|lstlisting)\\*?\\}\", \"\", text, flags=re.MULTILINE)\n",
    "        text = re.sub(r\"\\\\documentclass(?:\\[.*?\\])?\\{.*?\\}\", \"\", text, flags=re.DOTALL)\n",
    "        text = re.sub(r\"\\\\usepackage(?:\\[.*?\\])?\\{.*?\\}\", \"\", text, flags=re.DOTALL)\n",
    "        text = re.sub(r\"\\\\(title|author|date|thanks)\\{.*?\\}\", \"\", text, flags=re.DOTALL)\n",
    "        text = re.sub(r\"\\\\(maketitle|tableofcontents|listoffigures|listoftables|centering|newpage|section\\*|subsection\\*|subsubsection\\*)\\b\", \"\", text)\n",
    "        text = re.sub(r\"\\\\(begin|end)\\{.*?\\}\", \"\", text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "class SmartSheetProcessor:\n",
    "    def process_sheet(self, sheet_path: str) -> List[Document]:\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        try:\n",
    "            if sheet_path.lower().endswith(\".csv\"): loader = CSVLoader(file_path=sheet_path, encoding='utf-8')\n",
    "            elif sheet_path.lower().endswith(\".xlsx\"): loader = UnstructuredExcelLoader(sheet_path, mode=\"elements\")\n",
    "            else: return []\n",
    "            return loader.load()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {sheet_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "class SmartPPTProcessor:\n",
    "    def process_ppt(self, ppt_path: str) -> List[Document]:\n",
    "        print(f\"Processing PowerPoint: {ppt_path}\")\n",
    "        try:\n",
    "            loader = UnstructuredPowerPointLoader(ppt_path, mode=\"elements\")\n",
    "            return loader.load()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {ppt_path}: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_documents(embedding_function, llm_for_summaries) -> List[Document]:\n",
    "    \"\"\"Loads and processes all documents from the source folder.\"\"\"\n",
    "    print(\"--- 📂 Processing all source documents... ---\")\n",
    "    all_splits = []\n",
    "    processors = {\n",
    "        \".pdf\": SmartPDFProcessor(embeddings=embedding_function, llm=llm_for_summaries),\n",
    "        \".txt\": SmartDocProcessor(embeddings=embedding_function),\n",
    "        \".docx\": SmartDocProcessor(embeddings=embedding_function),\n",
    "        \".tex\": SmartLatexProcessor(embeddings=embedding_function),\n",
    "        \".csv\": SmartSheetProcessor(),\n",
    "        \".xlsx\": SmartSheetProcessor(),\n",
    "        \".pptx\": SmartPPTProcessor(),\n",
    "        \".ppt\": SmartPPTProcessor()\n",
    "    }\n",
    "    for filename in os.listdir(DOCS_PATH):\n",
    "        file_path = os.path.join(DOCS_PATH, filename)\n",
    "        file_ext = os.path.splitext(filename)[1].lower()\n",
    "        if file_ext in processors:\n",
    "            processor = processors[file_ext]\n",
    "            if hasattr(processor, 'process_pdf'): all_splits.extend(processor.process_pdf(file_path))\n",
    "            elif hasattr(processor, 'process_document'): all_splits.extend(processor.process_document(file_path))\n",
    "            elif hasattr(processor, 'process_latex'): all_splits.extend(processor.process_latex(file_path))\n",
    "            elif hasattr(processor, 'process_sheet'): all_splits.extend(processor.process_sheet(file_path))\n",
    "            elif hasattr(processor, 'process_ppt'): all_splits.extend(processor.process_ppt(file_path))\n",
    "    return all_splits\n",
    "\n",
    "def setup_hybrid_retriever(embedding_function, llm_for_summaries):\n",
    "    \"\"\"Initializes and returns a hybrid retriever (BM25 keyword search + Chroma vector search).\"\"\"\n",
    "    \n",
    "    all_documents = process_all_documents(embedding_function, llm_for_summaries)\n",
    "    if not all_documents:\n",
    "        raise ValueError(f\"No processable documents found in '{DOCS_PATH}'. Processing halted.\")\n",
    "\n",
    "    if os.path.exists(CHROMA_PERSIST_PATH) and os.listdir(CHROMA_PERSIST_PATH):\n",
    "        print(\"✅ Loading existing vector store...\")\n",
    "        vectorstore = Chroma(persist_directory=CHROMA_PERSIST_PATH, embedding_function=embedding_function)\n",
    "    else:\n",
    "        print(\"⚠️ No existing vector store found. Creating a new one...\")\n",
    "        vectorstore = Chroma.from_documents(documents=all_documents, embedding=embedding_function, persist_directory=CHROMA_PERSIST_PATH)\n",
    "        print(\"✅ Vector store created successfully.\")\n",
    "    \n",
    "    vector_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "    bm25_retriever = BM25Retriever.from_documents(all_documents, k=10)\n",
    "    \n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, vector_retriever],\n",
    "        weights=[0.5, 0.5]\n",
    "    )\n",
    "    return ensemble_retriever\n",
    "\n",
    "def create_conversational_rag_chain(retriever, llm):\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages([(\"system\", contextualize_q_system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\")])\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "    \n",
    "    qa_system_prompt = (\n",
    "        \"You are an expert AI Curriculum Assistant. Your task is to answer user questions accurately and concisely based ONLY on the provided context. This context contains text excerpts (extracted by Nougat) and detailed summaries of images, charts, or diagrams. When referencing visual content, explicitly mention it (e.g., 'As seen in the diagram on page 5...'). If the context does not contain the answer, state that you cannot find the information in the provided materials. Do not use any external knowledge.\\n\\nContext:\\n{context}\"\n",
    "    )\n",
    "    qa_prompt = ChatPromptTemplate.from_messages([(\"system\", qa_system_prompt), MessagesPlaceholder(\"chat_history\"), (\"human\", \"{input}\")])\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Initialization ---\n",
    "print(\"--- 🚀 AI Curriculum Assistant Initializing (with Nougat & Gemini) 🚀 ---\")\n",
    "rag_chain = None\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"❌ Error: GOOGLE_API_KEY not found. Please set it in your .env file.\")\n",
    "else:\n",
    "    try:\n",
    "        print(f\"Initializing embedding model for retrieval: '{EMBEDDING_MODEL}'\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "        \n",
    "        print(f\"Initializing Gemini LLM: '{LLM_MODEL}'\")\n",
    "        llm = ChatGoogleGenerativeAI(model=LLM_MODEL, google_api_key=GOOGLE_API_KEY, temperature=0.5)\n",
    "        \n",
    "        # ⬇️ MODIFIED: Pass the Gemini LLM to the retriever setup for image summarization\n",
    "        base_retriever = setup_hybrid_retriever(embeddings, llm)\n",
    "\n",
    "        print(f\"Initializing cross-encoder for reranking: '{CROSS_ENCODER_MODEL}'\")\n",
    "        cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL)\n",
    "        compressor = CrossEncoderReranker(model=cross_encoder, top_n=3)\n",
    "        \n",
    "        retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor, base_retriever=base_retriever\n",
    "        )\n",
    "        \n",
    "        rag_chain = create_conversational_rag_chain(retriever, llm)\n",
    "        chat_history = []\n",
    "        \n",
    "        print(\"\\n✅ Assistant is ready with Nougat PDF processing, Hybrid Search, and Reranking!\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An unexpected error occurred during initialization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceed45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Interactive Q&A (with Source Citation) ---\n",
    "user_input = \"What is the main topic of the documents?\"\n",
    "if rag_chain:\n",
    "    response = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
    "    answer = response[\"answer\"]\n",
    "    print(f\"🤖 Assistant: {answer}\")\n",
    "    \n",
    "    source_docs = response.get(\"context\", [])\n",
    "    if source_docs:\n",
    "        print(\"\\n--- 📚 Sources ---\")\n",
    "        unique_sources = set()\n",
    "        for doc in source_docs:\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            page = doc.metadata.get('page', 'N/A')\n",
    "            source_info = f\"📄 File: {os.path.basename(source)}, Page: {page}\"\n",
    "            # ⬇️ RE-ENABLED: Check for image summary metadata\n",
    "            if \"image_summary\" in doc.metadata.get(\"chunk_method\", \"\"):\n",
    "                source_info += \" (Image Summary)\"\n",
    "            unique_sources.add(source_info)\n",
    "        for src in sorted(list(unique_sources)):\n",
    "            print(src)\n",
    "    \n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    chat_history.append(AIMessage(content=answer))\n",
    "else:\n",
    "    print(\"The RAG chain is not initialized. Please run the previous cells successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edf6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Additional Assistant Functions (Unchanged) ---\n",
    "def find_connections(topic: str, retriever):\n",
    "    pass\n",
    "def summarize_document(doc_source_name: str, vectorstore, llm):\n",
    "    pass\n",
    "def generate_quiz(topic: str, retriever, llm, num_questions=3):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
